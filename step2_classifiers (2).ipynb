{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "threaded-coach",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table><tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td class=\"sessionId\">1</td><td>None</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"/spark/erin/40487/jobs\">Link</a></td><td></td><td>erin</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "'Field \"V1\" does not exist.'\n",
      "Traceback (most recent call last):\n",
      "  File \"/spark/python/lib/pyspark.zip/pyspark/ml/base.py\", line 173, in transform\n",
      "    return self._transform(dataset)\n",
      "  File \"/spark/python/lib/pyspark.zip/pyspark/ml/wrapper.py\", line 305, in _transform\n",
      "    return DataFrame(self._java_obj.transform(dataset._jdf), dataset.sql_ctx)\n",
      "  File \"/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 79, in deco\n",
      "    raise IllegalArgumentException(s.split(': ', 1)[1], stackTrace)\n",
      "pyspark.sql.utils.IllegalArgumentException: 'Field \"V1\" does not exist.'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "df=spark.sql(\"select * from user_erin.creditcard\")\n",
    "df_assembler = VectorAssembler(inputCols=['time','amount','V1','V2','V3','V4','V5','V6','V7','V8','V9'\n",
    "                                          ,'V10','V11','V12','V13','V14'\n",
    "                                         ,'V15','V16','V17','V18','V19','V20'\n",
    "                                         ,'V21','V22','V23','V24','V25','V26','V27','V28'],outputCol='features')\n",
    "data= df_assembler.transform(df)\n",
    "(train, test) = data.randomSplit([0.8, 0.2],seed = 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "sought-accident",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+-------------------+--------------------+------------------+------------------+------------------+--------------------+-------------------+------------------+------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+--------------------+--------------------+-------------------+-------------------+--------------------+--------------------+-----+--------------------+-------------------+--------------------+\n",
      "|                v1|                v2|                 v3|                  v4|                v5|                v6|                v7|                  v8|                 v9|               v10|               v11|                v12|                v13|                v14|                v15|                v16|                v17|                v18|                v19|                v20|                v21|                v22|                 v23|                 v24|                v25|                v26|                 v27|                 v28|class|              amount|               time|            features|\n",
      "+------------------+------------------+-------------------+--------------------+------------------+------------------+------------------+--------------------+-------------------+------------------+------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+--------------------+--------------------+-------------------+-------------------+--------------------+--------------------+-----+--------------------+-------------------+--------------------+\n",
      "| -1.43411607632232|0.0528693979077127|  -0.15970324721442|   -2.38524646437729| 0.837335020032831|-0.864148941647866|0.0587389308210593|    0.44363661821349|  0.382263734826317| -1.24639519705382| 0.738291730265881|   1.03833117879879|0.00165877680586613|  0.846831337210595|  0.642179433826407| -0.474418629782265| -0.264511498059065|  0.158878330752203|   0.85977484762451|-0.0351995012007581|  0.128404624907052|  0.308762403376689|  -0.416395983599717|  -0.740970345850072| -0.116687798420571|-0.0871274706693047|   0.194358109064051| -0.0205982495416478|  0.0| -0.3297313434930818|-1.9764298834319463|[-1.9764298834319...|\n",
      "|-0.792839369087318|  1.09842403509282|  0.617479132641594|   0.707670168991355| 0.940218751346935|-0.439779897730709|  1.73584331278293|  -0.728396902403134| -0.135613826747238|  0.92731156976663|  1.61619226343289|  0.161856239171295|   0.14446559962273|  -1.38200292724275|  0.433858962586313|-0.0713453873414164|0.00120344436377837|  0.897517067565052|  0.688854907329455|  0.629017074215728| -0.217902344289264|  0.272610293762688|  -0.230940970830165| -0.0796056853249647| -0.106818590218188|  -0.42259425055654|   0.158895773264595|  -0.223085833840655|  0.0|0.011028957052335234|-1.9690000016277078|[-1.9690000016277...|\n",
      "|  1.30397522781398|-0.258310215723446| -0.721635072123658|  -0.854540564249444|  1.70144959273884|   3.4259694338624| -1.01505276598956|   0.794229926752232|   1.59625336199495|-0.460929280189607| 0.877475031808206|  -2.51185852259019|   1.79837872499996|   1.60489694383758|  0.761679083568949|  0.810959722739774| -0.208712871027378|  0.593591444288345|-0.0308739217836815| 0.0446268050312315| -0.119766215312495| -0.231042470094357| -0.0506687534829966|    1.02196185266473|  0.368670366321438|   1.04626568062468| -0.0865588182483407|-9.83230482832512E-4|  0.0| -0.2776655582509288|-1.6934418923328312|[-1.6934418923328...|\n",
      "|  1.19464730290852|0.0327829014375688|   1.00566721715244|   0.238997752714476|-0.815749391382115|-0.669300641305602| -0.45376505165875| -0.0808411632385949|    1.3719858361269|-0.376533195437408|  2.62638884623442|  -1.44018928849304|   1.83489883746445|   1.69611201614036| -0.194441628957667|  0.637048921442833|  0.129336898045808|  0.255817950176459|-0.0337920526942834|-0.0752950557399963|  -0.27341955394146| -0.597820004273877|   0.171598561238887|   0.511290268198061|-0.0681508598908001|  0.718887506634315| -0.0928344442632363| 0.00178437772229276|  0.0| -0.2887132087395864|-1.6763931550483175|[-1.6763931550483...|\n",
      "|  1.07913292125406| -1.16665591511267|    1.6032770443565|   -1.49459812581897| -1.57832070400841|  1.09744978501935| -1.74617005155488|   0.622125075567044|   4.79224806567831| -2.20253592272627|  1.81749527284091| -0.717849853529107|  0.409105336985802|  0.738598485094856|  -1.21531934924222|  -1.92252679020605|   1.74844436237404|  0.229084344869472|  0.898774864831719| -0.293473344470223| 0.0484324673378731|   1.08485128894615|  -0.151214094894161|  -0.259635827699049|  0.528682387357098| -0.489830110323122|   0.158794998010985|  0.0107869318038652|  0.0|-0.33271719497650276|-1.6650904736633994|[-1.6650904736633...|\n",
      "|-0.714990388968324| 0.320220958604504|   2.64769501745948|   -1.74102106008418|-0.268904952386857| -0.22143114480653| 0.279362571811146| -0.0561641081928958|   2.68906156732696| -2.05902387959787| 0.456599310346698|  -2.54169597948736|  0.597531061230003|   1.11967483594908|  0.313041190449956| -0.649307876532146|  0.583558673364508|  0.181076878835762| -0.510608669095073| -0.145712284349552| -0.138569102954323|  0.137306025326365|  -0.326632263143056|-0.00199349733697333|  0.364607091530668| -0.809445945987574| -0.0733392050674568|  -0.156948037425196|  0.0|-0.28923573274918507|-1.5998211720346633|[-1.5998211720346...|\n",
      "|   1.2943765397957|  0.93938453951868|-0.0642034616299056|    2.47775610518808|  0.80441532407628|-0.282579600204475|   0.5405720519623|  -0.381104153780743| 0.0948152590887889| 0.330779803094499|-0.119004226791003|  -2.22775840188997|   3.02283711560864|   1.65018746699978| -0.609519499694223|  0.484849248830792| -0.122645857401714| -0.110294723207082| -0.782034610356735|  -0.10206382546407| -0.206767725042982| -0.299466014832596|  -0.210486797096331|  -0.470276025838867|   0.86251513715809|  0.077473680207764| -0.0590999807818074|-0.00220449134550951|  0.0|-0.32566312084692073|-1.5430130955767598|[-1.5430130955767...|\n",
      "| -13.1240790836202|  6.56166558975325|  -10.3120922949336|      1.660353932477| -8.20711698287328| -1.60435960539082| -5.15108997067648|    8.24602890022165|   1.01374015684382|  1.14125043567578|-0.500220065649218|  0.931667933057441|   1.80141582676469|   7.43956553239856| -0.820456235951331|    2.1340670293889|   3.54891888967971|    1.1990251791522| -0.304414379597645|-0.0298265496065825|  0.124874911804087| -0.625364809185747|  -0.644118971752612|    0.33313683177102|  0.241435858279191| -0.435954025104112| -0.0109590546594352|  -0.141546198727974|  0.0|  0.0397304544367192|-1.5375827570342853|[-1.5375827570342...|\n",
      "|-0.709382151501065| 0.119430734453322|   2.00987925245064|  -0.816581561899808| 0.683491102213316|   1.0588111468123| 0.195536007693242|   0.310087734291098|   1.85447172575609| -1.34174713178457|  1.11077306639876|  -2.57006925395818| 0.0547257640353531|   1.24178696888863| -0.463323969755909|  -1.60041891639427|   1.94164983013712|  -2.26949503684248|  -1.96414741931928|  -0.27104214653568|-0.0931184796888025|  0.364260750204644|-9.44405449101626E-4|  -0.648142859148432| -0.607538483871591|  0.940538122139315|   -0.10447578326087|  -0.131002506430558|  0.0|  -0.296140514304596|-1.5031274694605217|[-1.5031274694605...|\n",
      "|  1.18526262593192|-0.450906298831668|  0.380780873943236|   -1.18201167154889|-0.879193257995357| -1.04693167556033|-0.299364091122665|  -0.248148221524823|   2.75276403469733| -1.42191727752976| 0.478344909162487|  -2.25555228408431|   1.30656120397641|   1.72041765831523|   1.23972503495746| -0.109000696258567|  0.384793857796321|  0.333118429032083|  0.579215070416061|  0.017256119227437| -0.286547958452167| -0.683779308355685| -0.0134724663211673|  -0.015929665197156|  0.247920121593319| -0.173365924542604| -0.0309807598746713|  0.0250110310862712|  0.0|-0.03487850950526...|-1.4961185441324438|[-1.4961185441324...|\n",
      "|-0.432970636348465| 0.216587942694906|   3.16386464851017|-0.00207575061157442|  0.60561190668837|  1.04038544409232|  1.29629454009881|   -1.57674413049578|   1.84674422596202|   2.1941010154284| 0.679917710283735|  -1.09367793293482|  -1.64658091840741|  -1.90349756162939|  0.380454405387613| -0.565301113034727|   -1.1096907966729|  0.718541518631046|   1.81264827467468|  0.492834598731939| -0.321130164744988|     0.217766839197|  -0.524052026310436|  -0.491493533921298|  -0.57810957365962|  0.264374008272513|   -2.17059708540324|   -1.66519993165655|  0.0| -0.2812112568874912|-1.4488451318595832|[-1.4488451318595...|\n",
      "| -4.13348694580511| -9.08372337795179|  -2.33498517213031|    2.41073078089342| -3.21313684115929|  1.68493457452352|  2.92143258510087|  -0.300709577940551|  0.442272714272825| -1.50857617170289|  1.23917307765574|   1.16279004983735| -0.290043142475737|  0.566791377070126| -0.346328013848946| -0.373857483236003|  0.652189737772823| -0.387134125751136| -0.907502566755005|   5.33970460894571|   1.70507412527998| -0.942309355398441|   -2.44038731119367| -0.0493264720111537|  -0.71121702568262|  0.924045148976353|  -0.538547331463333|   0.456425023620168|  0.0|   9.814101901132945|-1.3755355615361753|[-1.3755355615361...|\n",
      "|  1.31336424365759|0.0785074050277037|  -1.29161017715527|  -0.484039748617382|  2.14033192024224|  3.18117166924895|-0.506491527233482|   0.805143169124163|-0.0739424458503442|-0.216917872898073| 0.131560596266759|-0.0281910447578133| 0.0112654570903958| -0.113167885293165|   1.29987001707385|  0.812663649777901| -0.495515788576404|  0.166249784711215|  0.175524358966493| 0.0305143826062908| -0.343336709300022|  -1.14792826774824|  0.0962395414951102|     0.9460653035335|  0.370113137780001|  0.110741922333798| -0.0216306540039611|  0.0234485892419911|  0.0| -0.3301418980720522|-1.3308510315917035|[-1.3308510315917...|\n",
      "|  1.57368692726108|-0.785619901280245| -0.472921665547401|   -1.65156080465007|-0.582824601200253|-0.538137126467103|-0.494722031467359|  -0.281909086687772|  -2.55342972042944|  1.63683919295286| 0.665335297072146| 0.0138526277231672|   1.43136139851761|-0.0819000613827652| -0.475271878926574| -0.385778147312149| 0.0581265077573809|  0.389364710450648|  0.468215041884488| -0.239780786279983| -0.162637635732047|-0.0505461132239826|  -0.209547907593146|  -0.471438981493502|  0.794712254734947|-0.0526179631500945|-0.00163558448330454|-0.00965397666067367|  0.0|  -0.277478942533215|-1.3272939493681446|[-1.3272939493681...|\n",
      "|-0.774417220042531|-0.937657047727363|   1.84050881131341|  -0.918334968927973| -2.59979082766476|  2.26985197998631| 0.541140902472208|   0.407531905100932| -0.248568963267261|-0.208943920192923| 0.883628006831944| -0.125024833591623| -0.872488039439863| -0.988014873266127|    -1.374010661376|  0.320868402925976|   1.12191260689415|  -1.53256117060766|   0.63314634880527| 0.0627374551077678|  0.258198446251464|    1.0028740499294|   0.171823895710803|  -0.239693157868767| -0.140050694290571| -0.120517690396367|    0.13640102527298| -0.0664362194939423|  0.0|   1.159462083863122|-1.3247050670397555|[-1.3247050670397...|\n",
      "|  1.29681475930938|-0.518507913625343|  0.348799998202684|   0.129749755298439|-0.845934138177093|-0.283286828522931|-0.463165294835426|-0.00734428346555328| -0.530097328062458| 0.611994015085672|-0.839572178028581| -0.238255463144304| -0.682034199965782| 0.0401067770158506|-0.0912699355568666|  -2.14411372225895|   0.65719915250189|  0.305925051561733| -0.899906172084595| -0.604167739239857| -0.498652056803025| -0.791893396278869|  0.0584562217711003|  0.0677528389721501|  0.367979257425209|   0.40089868564493| 0.00508927828085594| 0.00884587021389178|  0.0| -0.2812112568874912|-1.3047727778935405|[-1.3047727778935...|\n",
      "|-0.664040649051673| 0.552592405831023|    2.0388030566614|   0.235988275586168|-0.365727596896755|-0.359047958015468| 0.240790663988498|   0.101133772919108|  0.258404786131733|-0.487966312104767|-0.865689015312974|  0.328532795387468|  0.408759437189998| -0.530074608917994| -0.208430133513517| -0.172391566961788| -0.125575409922874|-0.0411652495946275| 0.0821800654015697|  0.211792928378534|  0.148661615764399|  0.688518873661724|  -0.153518100649602|   0.469412751642612|-0.0836071917855553|  0.452773518342167|   0.354109112845165|   0.201035227039579|  0.0| -0.2215315503626148|-1.2987951959320567|[-1.2987951959320...|\n",
      "|-0.269489597535828|-0.517466694811805|  0.715068242649305|   -1.61958377425802| 0.571551203105731| -1.01523397682374| 0.198985123122126|  -0.255847683355287|  -0.62267505596819| 0.352834696199914| -1.80133336449912|   -2.1350728131018|  -1.98865039758174| -0.156383122506121| -0.638596665687475|  0.902423195588495| 0.0546438432860315|  -1.05020362177836|  0.598538162894283|  0.068477155167454|  0.208082107672054|  0.567256332565023|   -0.19888910194116|  -0.473116480110767|  -0.27568447986443| -0.307826491794039| -0.0383850909156767| -0.0542199944772516|  0.0|  -0.314802086075977|-1.2705279685578572|[-1.2705279685578...|\n",
      "|  1.09514334727437|-0.234238940463285| 0.0855063118305372|   0.184883914063934|-0.587702059885687| -1.14993621405397| 0.244864184866234|  -0.308517175821152| 0.0811259202824891|-0.162936517164962| -0.24517189167356|  0.209383560809258|  0.174562987822514|  0.299022425037766|  0.994146583490469|  0.180017938745482| -0.276846897423587| -0.469789414967282|-0.0317941599475755|  0.148681796610877| 0.0391169855936067| -0.095608631639513|  -0.129414615903339|    0.48097222947283|  0.353280728703922|   1.02686199052487|  -0.110141164419084|  0.0182991136946606|  0.0| 0.08045000404187258|-1.2610143521966226|[-1.2610143521966...|\n",
      "|-0.440952701846983|-0.214163382587194|   1.87620803733271|  -0.559077705445447| -1.12089432541023|-0.361452479055247|-0.271772305400375|    0.12096435119329| -0.666661800941548| 0.327527361657264|-0.270529873280203|  -1.23314820869053|  -1.65184967158304|  -0.25966951064699|  0.168966354057338| 0.0359501812957877|   1.39804391177033|  -1.95076761977943|   1.20810652567165|  0.019985255008418| 0.0251448834460206| 0.0640602148425912|   0.255839783018824|   0.573950349596726| -0.813663935256552|  -0.44178333928831|   0.072704031215346|   0.173436522155785|  0.0|-0.16554683504847176|-1.2323472161700704|[-1.2323472161700...|\n",
      "+------------------+------------------+-------------------+--------------------+------------------+------------------+------------------+--------------------+-------------------+------------------+------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+--------------------+--------------------+-------------------+-------------------+--------------------+--------------------+-----+--------------------+-------------------+--------------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "reasonable-protocol",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "An error occurred while calling o275.count.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 111 in stage 10.0 failed 4 times, most recent failure: Lost task 111.3 in stage 10.0 (TID 492, 172.16.11.186, executor 0): org.apache.spark.SparkException: Failed to execute user defined function($anonfun$3: (struct<time:double,amount:double,v1:double,v2:double,v3:double,v4:double,v5:double,v6:double,v7:double,v8:double,v9:double,v10:double,v11:double,v12:double,v13:double,v14:double,v15:double,v16:double,v17:double,v18:double,v19:double,v20:double,v21:double,v22:double,... 6 more fields>) => vector)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.sort_addToSorter$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithoutKey$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$1.hasNext(WholeStageCodegenExec.scala:614)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.spark.SparkException: Values to assemble cannot be null.\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$assemble$1.apply(VectorAssembler.scala:163)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$assemble$1.apply(VectorAssembler.scala:146)\n",
      "\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:146)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$3.apply(VectorAssembler.scala:99)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$3.apply(VectorAssembler.scala:98)\n",
      "\t... 14 more\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1599)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1587)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1586)\n",
      "\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1586)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n",
      "\tat scala.Option.foreach(Option.scala:257)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1820)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1769)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1758)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2027)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2048)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2067)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2092)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:939)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n",
      "\tat org.apache.spark.rdd.RDD.collect(RDD.scala:938)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:297)\n",
      "\tat org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2770)\n",
      "\tat org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2769)\n",
      "\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3253)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\n",
      "\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3252)\n",
      "\tat org.apache.spark.sql.Dataset.count(Dataset.scala:2769)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.spark.SparkException: Failed to execute user defined function($anonfun$3: (struct<time:double,amount:double,v1:double,v2:double,v3:double,v4:double,v5:double,v6:double,v7:double,v8:double,v9:double,v10:double,v11:double,v12:double,v13:double,v14:double,v15:double,v16:double,v17:double,v18:double,v19:double,v20:double,v21:double,v22:double,... 6 more fields>) => vector)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.sort_addToSorter$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithoutKey$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$1.hasNext(WholeStageCodegenExec.scala:614)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "Caused by: org.apache.spark.SparkException: Values to assemble cannot be null.\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$assemble$1.apply(VectorAssembler.scala:163)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$assemble$1.apply(VectorAssembler.scala:146)\n",
      "\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:146)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$3.apply(VectorAssembler.scala:99)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$3.apply(VectorAssembler.scala:98)\n",
      "\t... 14 more\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py\", line 455, in count\n",
      "    return int(self._jdf.count())\n",
      "  File \"/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 63, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 328, in get_return_value\n",
      "    format(target_id, \".\", name), value)\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o275.count.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 111 in stage 10.0 failed 4 times, most recent failure: Lost task 111.3 in stage 10.0 (TID 492, 172.16.11.186, executor 0): org.apache.spark.SparkException: Failed to execute user defined function($anonfun$3: (struct<time:double,amount:double,v1:double,v2:double,v3:double,v4:double,v5:double,v6:double,v7:double,v8:double,v9:double,v10:double,v11:double,v12:double,v13:double,v14:double,v15:double,v16:double,v17:double,v18:double,v19:double,v20:double,v21:double,v22:double,... 6 more fields>) => vector)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.sort_addToSorter$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithoutKey$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$1.hasNext(WholeStageCodegenExec.scala:614)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.spark.SparkException: Values to assemble cannot be null.\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$assemble$1.apply(VectorAssembler.scala:163)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$assemble$1.apply(VectorAssembler.scala:146)\n",
      "\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:146)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$3.apply(VectorAssembler.scala:99)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$3.apply(VectorAssembler.scala:98)\n",
      "\t... 14 more\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1599)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1587)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1586)\n",
      "\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1586)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n",
      "\tat scala.Option.foreach(Option.scala:257)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1820)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1769)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1758)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2027)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2048)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2067)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2092)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:939)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n",
      "\tat org.apache.spark.rdd.RDD.collect(RDD.scala:938)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:297)\n",
      "\tat org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2770)\n",
      "\tat org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2769)\n",
      "\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3253)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\n",
      "\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3252)\n",
      "\tat org.apache.spark.sql.Dataset.count(Dataset.scala:2769)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.spark.SparkException: Failed to execute user defined function($anonfun$3: (struct<time:double,amount:double,v1:double,v2:double,v3:double,v4:double,v5:double,v6:double,v7:double,v8:double,v9:double,v10:double,v11:double,v12:double,v13:double,v14:double,v15:double,v16:double,v17:double,v18:double,v19:double,v20:double,v21:double,v22:double,... 6 more fields>) => vector)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.sort_addToSorter$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithoutKey$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$1.hasNext(WholeStageCodegenExec.scala:614)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "Caused by: org.apache.spark.SparkException: Values to assemble cannot be null.\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$assemble$1.apply(VectorAssembler.scala:163)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$assemble$1.apply(VectorAssembler.scala:146)\n",
      "\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:146)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$3.apply(VectorAssembler.scala:99)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$3.apply(VectorAssembler.scala:98)\n",
      "\t... 14 more\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "precise-royalty",
   "metadata": {},
   "source": [
    "## 机器学习：评估方程\n",
    "- 运用BinaryClassificationEvaluator，得到每个预测结果的AUC。\n",
    "- 对于二分类的数据集，得到accuracy，precision，recall，f1_score。\n",
    "- 本数据集的标签为1和0，您可以通过改变等号后的数据对于其他二分类预测结果进行评估。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "induced-translator",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "functional-nudist",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pyspark.sql.functions as f\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"class\", rawPredictionCol=\"prediction\", metricName='areaUnderROC')\n",
    "def precise_recall_f1(prediction):\n",
    "    tp = prediction.filter((f.col('class')  == 1) & (f.col('prediction')  == 1)).count()\n",
    "    tn = prediction.filter((f.col('class')  == 0) & (f.col('prediction')  == 0)).count()\n",
    "    fp = prediction.filter((f.col('class')  == 0) & (f.col('prediction')  == 1)).count()\n",
    "    fn = prediction.filter((f.col('class')  == 1) & (f.col('prediction')  == 0)).count()\n",
    "    try:\n",
    "        acc=float(tp+tn)/(tp+tn+fp+fn)\n",
    "    except:\n",
    "        acc=0\n",
    "    try:\n",
    "        p = float(tp)/(tp + fp)\n",
    "    except:\n",
    "        p = 0\n",
    "    try:\n",
    "        r = float(tp)/(tp + fn)\n",
    "    except:\n",
    "        r = 0\n",
    "    try:\n",
    "        f1 = 2*p*r/(p+r)\n",
    "    except:\n",
    "        f1 = 0\n",
    "    result=[acc,p,r,f1]\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "healthy-register",
   "metadata": {},
   "source": [
    "## 机器学习：分类模型\n",
    "- 以下分别使用了逻辑回归，决策树分类，随机森林分类，梯度提升决策树分类，线性支持向量分类。\n",
    "- 将五种分类模型与数据集进行拟合并且完成预测，将预测的评估结果打印在每个单元格下。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floating-turtle",
   "metadata": {},
   "source": [
    "逻辑回归（Logistic Regression）是一种用于解决二分类（0 or 1）问题的机器学习方法，用于估计某种事物的可能性。逻辑回归假设因变量 y 服从伯努利分布。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "talented-beach",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "name 'train' is not defined\n",
      "Traceback (most recent call last):\n",
      "NameError: name 'train' is not defined\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(featuresCol='features',labelCol = 'class').fit(train)\n",
    "lrpredictions= lr.transform(test)\n",
    "lrresult=['lr']\n",
    "lrresult1=precise_recall_f1(lrpredictions)\n",
    "lrresult=lrresult+lrresult1\n",
    "lrroc=evaluator.evaluate(lrpredictions)\n",
    "lrresult.append(lrroc)\n",
    "print(\"lrresult\",lrresult)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "equipped-recipient",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dtresult ['dt', 0.9993835966256318, 0.9397590361445783, 0.7222222222222222, 0.8167539267015707, 0.8610669983942972]"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "dt = DecisionTreeClassifier(featuresCol='features',labelCol = 'class')\n",
    "dtmodel= dt.fit(train)\n",
    "dtpredictions=dtmodel.transform(test)\n",
    "dtresult=['dt']\n",
    "dtresult1=precise_recall_f1(dtpredictions)\n",
    "dtresult=dtresult+dtresult1\n",
    "dtroc=evaluator.evaluate(dtpredictions)\n",
    "dtresult.append(dtroc)\n",
    "print(\"dtresult\",dtresult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "wrong-monthly",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rfresult ['rf', 0.9993659851006499, 0.8829787234042553, 0.7685185185185185, 0.8217821782178217, 0.8841622112822686]"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(labelCol=\"Class\", featuresCol=\"features\", numTrees=10)\n",
    "rfmodel= rf.fit(train)\n",
    "rfpredictions=rfmodel.transform(test)\n",
    "rfresult=['rf']\n",
    "rfresult1=precise_recall_f1(rfpredictions)\n",
    "rfresult=rfresult+rfresult1\n",
    "rfroc=evaluator.evaluate(rfpredictions)\n",
    "rfresult.append(rfroc)\n",
    "print(\"rfresult\",rfresult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "sporting-remove",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gbtresult ['gbt', 0.9993835966256318, 0.9101123595505618, 0.75, 0.8223350253807107, 0.8749294196530977]"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import GBTClassifier\n",
    "gbt = GBTClassifier(labelCol=\"Class\", featuresCol=\"features\", maxIter=10)\n",
    "gbtmodel=gbt.fit(train)\n",
    "gbtpredictions=gbtmodel.transform(test)\n",
    "gbtresult=['gbt']\n",
    "gbtresult1=precise_recall_f1(gbtpredictions)\n",
    "gbtresult=gbtresult+gbtresult1\n",
    "gbtroc=evaluator.evaluate(gbtpredictions)\n",
    "gbtresult.append(gbtroc)\n",
    "print(\"gbtresult\",gbtresult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "athletic-accounting",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lsvcresult ['lsvc', 0.9992250929007943, 0.9102564102564102, 0.6574074074074074, 0.7634408602150539, 0.8286419459001642]"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LinearSVC\n",
    "lsvc = LinearSVC(labelCol=\"Class\", featuresCol=\"features\", maxIter=10, regParam=0.1)\n",
    "lsvcModel = lsvc.fit(train)\n",
    "lsvcpredictions=lsvcModel.transform(test)\n",
    "lsvcresult=['lsvc']\n",
    "lsvcresult1=precise_recall_f1(lsvcpredictions)\n",
    "lsvcresult=lsvcresult+lsvcresult1\n",
    "lsvcroc=evaluator.evaluate(lsvcpredictions)\n",
    "lsvcresult.append(lsvcroc)\n",
    "print(\"lsvcresult\",lsvcresult)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surrounded-handbook",
   "metadata": {},
   "source": [
    "## 机器学习：评估表格\n",
    "- 以下的表格包含了五个分类模型的五个参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "connected-sarah",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------+------------------+------------------+------------------+------------------+\n",
      "|    |               acc|         precision|            recall|                f1|               auc|\n",
      "+----+------------------+------------------+------------------+------------------+------------------+\n",
      "|  lr|0.9992603159507581|            0.9125|0.6759259259259259|0.7765957446808511|0.8379012051594233|\n",
      "|  dt|0.9993835966256318|0.9397590361445783|0.7222222222222222|0.8167539267015707|0.8610669983942972|\n",
      "|  rf|0.9993659851006499|0.8829787234042553|0.7685185185185185|0.8217821782178217|0.8841622112822686|\n",
      "| gbt|0.9993835966256318|0.9101123595505618|              0.75|0.8223350253807107|0.8749294196530977|\n",
      "|lsvc|0.9992250929007943|0.9102564102564102|0.6574074074074074|0.7634408602150539|0.8286419459001642|\n",
      "+----+------------------+------------------+------------------+------------------+------------------+"
     ]
    }
   ],
   "source": [
    "result=[lrresult,dtresult,rfresult,gbtresult,lsvcresult]\n",
    "rdf=spark.createDataFrame(result)\n",
    "rdf=rdf.toDF(\"\", \"acc\", \"precision\", \"recall\",\"f1\",\"auc\")\n",
    "rdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fixed-lawsuit",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "martial-diagram",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lr.write().overwrite().save(\"hdfs:///user/erin/data1/gbtcmodel\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fossil-witness",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
