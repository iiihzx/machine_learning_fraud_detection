{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "moving-spouse",
   "metadata": {},
   "source": [
    "## 机器学习：将数据导入并分成测试集与训练集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "geographic-permit",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.ml.classification import GBTClassificationModel\n",
    "df=spark.sql(\"select * from user_erin.creditcard\")\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "df_assembler = VectorAssembler(inputCols=['time','V1','V2','V3','V4','V5','V6','V7','V8','V9'\n",
    "                                          ,'V10','V11','V12','V13','V14'\n",
    "                                         ,'V15','V16','V17','V18','V19','V20'\n",
    "                                         ,'V21','V22','V23','V24','V25','V26','V27','V28','amount'],outputCol='features')\n",
    "df= df_assembler.transform(df)\n",
    "df=df.withColumnRenamed('class', 'label')\n",
    "(train, test) = df.randomSplit([0.8, 0.2],seed = 11)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "million-tonight",
   "metadata": {},
   "source": [
    "## 机器学习：GBTC模型的调参与预测\n",
    "- 用于分类的梯度提升决策树模型。该模型属于集成模型（Ensemble methods）家族。集成模型结合多个弱预测模型而形成一个强健的模型。\n",
    "- 运用TrainValidationSplit（TVS）来进行参数调优。TVS对于每个参数组合只进行一次评估，当训练数据集足够大时，开销较小，并且产生的结果可靠。TVS会创建单个 (training, test) pair。它使用trainRatio参数将数据集split成两部分。例如： trainRatio=0.8，TVS会生成一个训练集（80%）和一个测试集（20%）。TVS最后会使用Estimator、以及最好的ParamMap，对整个数据集进行拟合。\n",
    "- 运用param_grid方程来定义TVS检测的参数，最后一共12种参数组合模型被测试。\n",
    "- maxIter 设置的越大结果越精确，但是设置的越大也就意味着越耗时；一般建议10-20。\n",
    "- maxDepth - 树的最大深度（例如深度0表示1个叶节点，深度1表示1个内部节点+ 2个叶节点）。 （默认值：3）\n",
    "- maxBins - 用于分割特征的最大bin数量。 DecisionTree需要maxBins> = max类别。 （默认值：32）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ambient-organization",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:0.9994012081506137"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "import pyspark.sql.functions as f\n",
    "gbt = GBTClassifier()\n",
    "gbt_evaluator = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"prediction\", metricName='areaUnderROC')\n",
    "param_grid = ParamGridBuilder() \\\n",
    "            .addGrid(gbt.maxDepth, [ 2, 4, 6]) \\\n",
    "            .addGrid(gbt.maxIter,[10,20])\\\n",
    "            .addGrid(gbt.maxBins,[20,60])\\\n",
    "            .build()\n",
    "tvs = TrainValidationSplit(estimator=gbt,\n",
    "                           estimatorParamMaps=param_grid,\n",
    "                           evaluator=gbt_evaluator,\n",
    "                           trainRatio = 0.8)\n",
    "model=tvs.fit(train)\n",
    "model=model.bestModel\n",
    "prediction=model.transform(test)\n",
    "tp = prediction.filter((f.col('label')  == 1) & (f.col('prediction')  == 1)).count()\n",
    "tn = prediction.filter((f.col('label')  == 0) & (f.col('prediction')  == 0)).count()\n",
    "fp = prediction.filter((f.col('label')  == 0) & (f.col('prediction')  == 1)).count()\n",
    "fn = prediction.filter((f.col('label')  == 1) & (f.col('prediction')  == 0)).count()\n",
    "acc=float(tp+tn)/(tp+tn+fp+fn)\n",
    "print( \"Accuracy:\" + str(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "union-vatican",
   "metadata": {},
   "source": [
    "将训练完成的表现最佳模型存入hdfs以便于后续的使用。\n",
    "将最佳模型的预测存入hive表中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "framed-sunrise",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.write().overwrite().save(\"hdfs:///user/erin/data/mllib/gbtcmodel\") \n",
    "prediction.write.saveAsTable(\"user_erin.creditcard_prediction\", format=\"orc\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "liked-bathroom",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
