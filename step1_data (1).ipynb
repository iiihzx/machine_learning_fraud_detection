{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "supposed-duplicate",
   "metadata": {},
   "source": [
    "## 数据导入：\n",
    "- 以下这个单元格的功能是将您上传的csv文件读入到pyspark程序里，并将其的格式转换为pyspark dataframe。\n",
    "- 您可以通过改变spark.read.csv()中的路径来读入您本人上传的数据集。\n",
    "- data.show()是将dataframe的内容展示出来的方程。展示出来的数据集默认是第一行，您可以通过在show（）中填写数字，来改变展示的行数。例如show（1），会展示数据集中的前1行，而不填数字会默认展示前20行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "optical-creator",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------+-------------------+----------------+------------------+-------------------+------------------+-----------------+-------------------+------------------+------------------+-------------------+------------------+------------------+-------------------+-------------------+-------------------+-----------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+-------------------+------------------+-------------------+------+-----+\n",
      "|Time|                V1|                 V2|              V3|                V4|                 V5|                V6|               V7|                 V8|                V9|               V10|                V11|               V12|               V13|                V14|                V15|                V16|              V17|               V18|               V19|               V20|               V21|               V22|               V23|               V24|               V25|                V26|               V27|                V28|Amount|Class|\n",
      "+----+------------------+-------------------+----------------+------------------+-------------------+------------------+-----------------+-------------------+------------------+------------------+-------------------+------------------+------------------+-------------------+-------------------+-------------------+-----------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+-------------------+------------------+-------------------+------+-----+\n",
      "| 0.0|  -1.3598071336738|-0.0727811733098497|2.53634673796914|  1.37815522427443| -0.338320769942518| 0.462387777762292|0.239598554061257| 0.0986979012610507| 0.363786969611213|0.0907941719789316| -0.551599533260813|-0.617800855762348|-0.991389847235408| -0.311169353699879|   1.46817697209427| -0.470400525259478|0.207971241929242|0.0257905801985591| 0.403992960255733| 0.251412098239705|-0.018306777944153| 0.277837575558899|-0.110473910188767|0.0669280749146731| 0.128539358273528| -0.189114843888824| 0.133558376740387|-0.0210530534538215|149.62|    0|\n",
      "| 7.0|-0.644269442348146|   1.41796354547385| 1.0743803763556|-0.492199018495015|  0.948934094764157| 0.428118462833089| 1.12063135838353|  -3.80786423873589| 0.615374730667027|  1.24937617815176| -0.619467796121913| 0.291474353088705|  1.75796421396042|  -1.32386521970526|  0.686132504394383|-0.0761269994382006| -1.2221273453247|-0.358221569869078| 0.324504731321494|-0.156741852488285|  1.94346533978412| -1.01545470979971| 0.057503529867291|-0.649709005559993|-0.415266566234811|-0.0516342969262494| -1.20692108094258|  -1.08533918832377|  40.8|    0|\n",
      "|15.0|   1.4929359769862|  -1.02934573189487|0.45479473374366| -1.43802587991702|  -1.55543410136344|-0.720961147043557|-1.08066413038614|-0.0531271179483221|  -1.9786815953872|  1.63807603690446|   1.07754241162743| -0.63204651464934| -0.41695716661602| 0.0520105153724404|-0.0429789228232019| -0.166432496451972|0.304241418614353| 0.554432499062278|0.0542295152184719|-0.387910172646258|-0.177649846438814|-0.175073809074822|0.0400022190621329| 0.295813862676508|  0.33293059939425| -0.220384850672322|0.0222984359135846|0.00760225559997897|   5.0|    0|\n",
      "|22.0|  -2.0742946722629| -0.121481799450951|1.32202063048967| 0.410007514171835|  0.295197545759436| -0.95953722984438|0.543985491287656| -0.104626728092018| 0.475664017945495| 0.149450615348245| -0.856566363963256|-0.180523156037298|-0.655232930357476|  -0.27979685563853|  -0.21166795514315| -0.333320609694671|0.010751094250554|-0.488472666295676| 0.505751034478604|-0.386693573241769|-0.403639498840125|-0.227404004096502| 0.742434864076795| 0.398534855447456| 0.249212161486784|  0.274404273874965| 0.359969356358436|  0.243231671798911| 26.43|    0|\n",
      "|23.0|-0.414288810090829|  0.905437322625407|1.72745294417921|  1.47347126657189|0.00744274117322988|-0.200330677416199|0.740228319420026| -0.029247400012072|-0.593392019124765|-0.346188231273199|-0.0121421884961019| 0.786796316060126| 0.635953883297271|-0.0863244719500975| 0.0768036871984215|   -1.4059193336148|0.775591738395162| -0.94288892749919| 0.543969461627011|0.0973075910928853|0.0772374339988386| 0.457330598656119|-0.038499724582026| 0.642521902774992|-0.183891335259639| -0.277464019495649| 0.182687486490454|  0.152664644968975|  33.0|    0|\n",
      "+----+------------------+-------------------+----------------+------------------+-------------------+------------------+-----------------+-------------------+------------------+------------------+-------------------+------------------+------------------+-------------------+-------------------+-------------------+-----------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+-------------------+------------------+-------------------+------+-----+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "data=spark.read.csv('hdfs://default/user/erin/demo/creditcarddemo.csv', header=True)\n",
    "data.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "chubby-dancing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------+-------------------+----------------+------------------+------------------+------------------+-----------------+-------------------+-----------------+------------------+------------------+------------------+------------------+------------------+-------------------+-------------------+-----------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+-------------------+------------------+-------------------+------+-----+\n",
      "|time|                v1|                 v2|              v3|                v4|                v5|                v6|               v7|                 v8|               v9|               v10|               v11|               v12|               v13|               v14|                v15|                v16|              v17|               v18|               v19|               v20|               v21|               v22|               v23|               v24|               v25|                v26|               v27|                v28|amount|class|\n",
      "+----+------------------+-------------------+----------------+------------------+------------------+------------------+-----------------+-------------------+-----------------+------------------+------------------+------------------+------------------+------------------+-------------------+-------------------+-----------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+-------------------+------------------+-------------------+------+-----+\n",
      "|Time|                V1|                 V2|              V3|                V4|                V5|                V6|               V7|                 V8|               V9|               V10|               V11|               V12|               V13|               V14|                V15|                V16|              V17|               V18|               V19|               V20|               V21|               V22|               V23|               V24|               V25|                V26|               V27|                V28|Amount|Class|\n",
      "| 0.0|  -1.3598071336738|-0.0727811733098497|2.53634673796914|  1.37815522427443|-0.338320769942518| 0.462387777762292|0.239598554061257| 0.0986979012610507|0.363786969611213|0.0907941719789316|-0.551599533260813|-0.617800855762348|-0.991389847235408|-0.311169353699879|   1.46817697209427| -0.470400525259478|0.207971241929242|0.0257905801985591| 0.403992960255733| 0.251412098239705|-0.018306777944153| 0.277837575558899|-0.110473910188767|0.0669280749146731| 0.128539358273528| -0.189114843888824| 0.133558376740387|-0.0210530534538215|149.62|    0|\n",
      "| 7.0|-0.644269442348146|   1.41796354547385| 1.0743803763556|-0.492199018495015| 0.948934094764157| 0.428118462833089| 1.12063135838353|  -3.80786423873589|0.615374730667027|  1.24937617815176|-0.619467796121913| 0.291474353088705|  1.75796421396042| -1.32386521970526|  0.686132504394383|-0.0761269994382006| -1.2221273453247|-0.358221569869078| 0.324504731321494|-0.156741852488285|  1.94346533978412| -1.01545470979971| 0.057503529867291|-0.649709005559993|-0.415266566234811|-0.0516342969262494| -1.20692108094258|  -1.08533918832377|  40.8|    0|\n",
      "|15.0|   1.4929359769862|  -1.02934573189487|0.45479473374366| -1.43802587991702| -1.55543410136344|-0.720961147043557|-1.08066413038614|-0.0531271179483221| -1.9786815953872|  1.63807603690446|  1.07754241162743| -0.63204651464934| -0.41695716661602|0.0520105153724404|-0.0429789228232019| -0.166432496451972|0.304241418614353| 0.554432499062278|0.0542295152184719|-0.387910172646258|-0.177649846438814|-0.175073809074822|0.0400022190621329| 0.295813862676508|  0.33293059939425| -0.220384850672322|0.0222984359135846|0.00760225559997897|   5.0|    0|\n",
      "|22.0|  -2.0742946722629| -0.121481799450951|1.32202063048967| 0.410007514171835| 0.295197545759436| -0.95953722984438|0.543985491287656| -0.104626728092018|0.475664017945495| 0.149450615348245|-0.856566363963256|-0.180523156037298|-0.655232930357476| -0.27979685563853|  -0.21166795514315| -0.333320609694671|0.010751094250554|-0.488472666295676| 0.505751034478604|-0.386693573241769|-0.403639498840125|-0.227404004096502| 0.742434864076795| 0.398534855447456| 0.249212161486784|  0.274404273874965| 0.359969356358436|  0.243231671798911| 26.43|    0|\n",
      "+----+------------------+-------------------+----------------+------------------+------------------+------------------+-----------------+-------------------+-----------------+------------------+------------------+------------------+------------------+------------------+-------------------+-------------------+-----------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+-------------------+------------------+-------------------+------+-----+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "df=spark.sql(\"select * from user_erin.creditcarddemo325\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "induced-amateur",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success"
     ]
    }
   ],
   "source": [
    "for col in df.columns:\n",
    "    df= df.withColumnRenamed(col, col.lower())\n",
    "print('success')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "median-toyota",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success"
     ]
    }
   ],
   "source": [
    "for col in data.columns:\n",
    "    data= data.withColumnRenamed(col, col.lower())\n",
    "print('success')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "governing-camping",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28697"
     ]
    }
   ],
   "source": [
    "data=data.distinct()\n",
    "data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "consistent-hawaii",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28698"
     ]
    }
   ],
   "source": [
    "df=df.distinct()\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "rising-roots",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+-----------------+-----------------+-----------------+----------------+----------------+-----------------+----------------+------------------+-----------------+-----------------+-----------------+------------------+------------------+----------------+-----------------+-----------------+-----------------+------------------+-------------------+-----------------+-----------------+------------------+-----------------+----------------+-----------------+------------------+------------------+------+-----+\n",
      "|    time|                v1|               v2|               v3|               v4|              v5|              v6|               v7|              v8|                v9|              v10|              v11|              v12|               v13|               v14|             v15|              v16|              v17|              v18|               v19|                v20|              v21|              v22|               v23|              v24|             v25|              v26|               v27|               v28|amount|class|\n",
      "+--------+------------------+-----------------+-----------------+-----------------+----------------+----------------+-----------------+----------------+------------------+-----------------+-----------------+-----------------+------------------+------------------+----------------+-----------------+-----------------+-----------------+------------------+-------------------+-----------------+-----------------+------------------+-----------------+----------------+-----------------+------------------+------------------+------+-----+\n",
      "|172768.0|-0.669661752830995|0.923768896481075|-1.54316735695144|-1.56072922562957|2.83396008646802|3.24084285526923|0.181575940386633|1.28274561286533|-0.893890143065396|-1.45343220956775|0.187487639015545|-0.39079405906916|-0.289170669489924|-0.510320078179671|0.95563741804494|0.553781107854912|0.567862192618873|0.409517378236063|-0.671301284193127|9.65375982347624E-4|0.183856197684746|0.202670336405986|-0.373022532745989|0.651121566749473|1.07382283272575|0.844590141858931|-0.286676371096233|-0.187719026467479|  40.0|    0|\n",
      "+--------+------------------+-----------------+-----------------+-----------------+----------------+----------------+-----------------+----------------+------------------+-----------------+-----------------+-----------------+------------------+------------------+----------------+-----------------+-----------------+-----------------+------------------+-------------------+-----------------+-----------------+------------------+-----------------+----------------+-----------------+------------------+------------------+------+-----+"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "expr = [F.last(col).alias(col) for col in data.columns]\n",
    "\n",
    "df.agg(*expr).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "roman-theta",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+-----------------+-----------------+-----------------+----------------+----------------+-----------------+----------------+------------------+-----------------+-----------------+-----------------+------------------+------------------+----------------+-----------------+-----------------+-----------------+------------------+-------------------+-----------------+-----------------+------------------+-----------------+----------------+-----------------+------------------+------------------+------+-----+\n",
      "|    time|                v1|               v2|               v3|               v4|              v5|              v6|               v7|              v8|                v9|              v10|              v11|              v12|               v13|               v14|             v15|              v16|              v17|              v18|               v19|                v20|              v21|              v22|               v23|              v24|             v25|              v26|               v27|               v28|amount|class|\n",
      "+--------+------------------+-----------------+-----------------+-----------------+----------------+----------------+-----------------+----------------+------------------+-----------------+-----------------+-----------------+------------------+------------------+----------------+-----------------+-----------------+-----------------+------------------+-------------------+-----------------+-----------------+------------------+-----------------+----------------+-----------------+------------------+------------------+------+-----+\n",
      "|172768.0|-0.669661752830995|0.923768896481075|-1.54316735695144|-1.56072922562957|2.83396008646802|3.24084285526923|0.181575940386633|1.28274561286533|-0.893890143065396|-1.45343220956775|0.187487639015545|-0.39079405906916|-0.289170669489924|-0.510320078179671|0.95563741804494|0.553781107854912|0.567862192618873|0.409517378236063|-0.671301284193127|9.65375982347624E-4|0.183856197684746|0.202670336405986|-0.373022532745989|0.651121566749473|1.07382283272575|0.844590141858931|-0.286676371096233|-0.187719026467479|  40.0|    0|\n",
      "+--------+------------------+-----------------+-----------------+-----------------+----------------+----------------+-----------------+----------------+------------------+-----------------+-----------------+-----------------+------------------+------------------+----------------+-----------------+-----------------+-----------------+------------------+-------------------+-----------------+-----------------+------------------+-----------------+----------------+-----------------+------------------+------------------+------+-----+"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "expr = [F.last(col).alias(col) for col in df.columns]\n",
    "\n",
    "df.agg(*expr).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "controversial-dynamics",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(time='3762.0', v1='1.39750906577018', v2='-0.69018341703715', v3='-0.917329301742026', v4='-1.61471962965142', v5='1.4563104543284', v6='3.27546246820649', v7='-1.16774157498849', v8='0.709839295313415', v9='0.35987413409625', v10='0.281089792873408', v11='1.00954847812403', v12='-3.17325618780044', v13='1.86952974687157', v14='1.41298089030819', v15='0.260107027719764', v16='1.53386811837655', v17='0.446125348008787', v18='-0.865424743599043', v19='0.765948526764106', v20='0.226103899669006', v21='-0.276905752278065', v22='-0.8993077895563', v23='0.0794922065648755', v24='0.92012311326219', v25='0.356299744230986', v26='-0.508593330847767', v27='-0.0203057183283516', v28='0.0157865402635591', amount='43.95', class='0')]"
     ]
    }
   ],
   "source": [
    "data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "temporal-truth",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(time='Time', v1='V1', v2='V2', v3='V3', v4='V4', v5='V5', v6='V6', v7='V7', v8='V8', v9='V9', v10='V10', v11='V11', v12='V12', v13='V13', v14='V14', v15='V15', v16='V16', v17='V17', v18='V18', v19='V19', v20='V20', v21='V21', v22='V22', v23='V23', v24='V24', v25='V25', v26='V26', v27='V27', v28='V28', amount='Amount', class='Class')]"
     ]
    }
   ],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bored-nothing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+----------+----------+----------+----------+----------+----------+----------+----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+--------------+-------------+\n",
      "|time_missing|v1_missing|v2_missing|v3_missing|v4_missing|v5_missing|v6_missing|v7_missing|v8_missing|v9_missing|v10_missing|v11_missing|v12_missing|v13_missing|v14_missing|v15_missing|v16_missing|v17_missing|v18_missing|v19_missing|v20_missing|v21_missing|v22_missing|v23_missing|v24_missing|v25_missing|v26_missing|v27_missing|v28_missing|amount_missing|class_missing|\n",
      "+------------+----------+----------+----------+----------+----------+----------+----------+----------+----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+--------------+-------------+\n",
      "|         0.0|       0.0|       0.0|       0.0|       0.0|       0.0|       0.0|       0.0|       0.0|       0.0|        0.0|        0.0|        0.0|        0.0|        0.0|        0.0|        0.0|        0.0|        0.0|        0.0|        0.0|        0.0|        0.0|        0.0|        0.0|        0.0|        0.0|        0.0|        0.0|           0.0|          0.0|\n",
      "+------------+----------+----------+----------+----------+----------+----------+----------+----------+----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+--------------+-------------+\n",
      "\n",
      "success"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as f\n",
    "data.agg(*[(1-(f.count(c) /f.count('*'))).alias(c+'_missing') for c in data.columns]).show()\n",
    "data=data.na.drop(subset=['class'])\n",
    "data=data.dropna(thresh=5)\n",
    "print('success')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "circular-console",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+----------+----------+----------+----------+----------+----------+----------+----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+--------------+-------------+\n",
      "|time_missing|v1_missing|v2_missing|v3_missing|v4_missing|v5_missing|v6_missing|v7_missing|v8_missing|v9_missing|v10_missing|v11_missing|v12_missing|v13_missing|v14_missing|v15_missing|v16_missing|v17_missing|v18_missing|v19_missing|v20_missing|v21_missing|v22_missing|v23_missing|v24_missing|v25_missing|v26_missing|v27_missing|v28_missing|amount_missing|class_missing|\n",
      "+------------+----------+----------+----------+----------+----------+----------+----------+----------+----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+--------------+-------------+\n",
      "|         0.0|       0.0|       0.0|       0.0|       0.0|       0.0|       0.0|       0.0|       0.0|       0.0|        0.0|        0.0|        0.0|        0.0|        0.0|        0.0|        0.0|        0.0|        0.0|        0.0|        0.0|        0.0|        0.0|        0.0|        0.0|        0.0|        0.0|        0.0|        0.0|           0.0|          0.0|\n",
      "+------------+----------+----------+----------+----------+----------+----------+----------+----------+----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+--------------+-------------+\n",
      "\n",
      "success"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as f\n",
    "df.agg(*[(1-(f.count(c) /f.count('*'))).alias(c+'_missing') for c in df.columns]).show()\n",
    "df=df.na.drop(subset=['class'])\n",
    "df=df.dropna(thresh=5)\n",
    "print('success')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "social-berkeley",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[time: double, v1: double, v2: double, v3: double, v4: double, v5: double, v6: double, v7: double, v8: double, v9: double, v10: double, v11: double, v12: double, v13: double, v14: double, v15: double, v16: double, v17: double, v18: double, v19: double, v20: double, v21: double, v22: double, v23: double, v24: double, v25: double, v26: double, v27: double, v28: double, amount: double, class: double]"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "colNames = data.columns\n",
    "for colName in colNames:\n",
    "    data = data.withColumn(colName, col(colName).cast('double'))\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "prospective-holmes",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[time: double, v1: double, v2: double, v3: double, v4: double, v5: double, v6: double, v7: double, v8: double, v9: double, v10: double, v11: double, v12: double, v13: double, v14: double, v15: double, v16: double, v17: double, v18: double, v19: double, v20: double, v21: double, v22: double, v23: double, v24: double, v25: double, v26: double, v27: double, v28: double, amount: double, class: double]"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "colNames = df.columns\n",
    "for colName in colNames:\n",
    "    df = df.withColumn(colName, col(colName).cast('double'))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "developmental-withdrawal",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import stddev, mean\n",
    "mean_amount, sttdev_amount = data.select(mean(\"amount\"), stddev(\"amount\")).first()\n",
    "data=data.withColumn(\"amount_scaled\", (col(\"amount\") - mean_amount) / sttdev_amount)\n",
    "mean_time, sttdev_time = data.select(mean(\"time\"), stddev(\"time\")).first()\n",
    "data=data.withColumn(\"time_scaled\", (col(\"time\") - mean_time) / sttdev_time)\n",
    "data=data.drop(\"time\",\"amount\")\n",
    "data=data.withColumnRenamed('time_scaled', 'time')\n",
    "data=data.withColumnRenamed('amount_scaled', 'amount')\n",
    "print('success')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "restricted-checklist",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success"
     ]
    }
   ],
   "source": [
    "data.write.format(\"hive\").mode(\"overwrite\").saveAsTable(\"user_erin.c1\")\n",
    "print('success')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "advised-printing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[time: double, v1: double, v2: double, v3: double, v4: double, v5: double, v6: double, v7: double, v8: double, v9: double, v10: double, v11: double, v12: double, v13: double, v14: double, v15: double, v16: double, v17: double, v18: double, v19: double, v20: double, v21: double, v22: double, v23: double, v24: double, v25: double, v26: double, v27: double, v28: double, amount: double, class: double]"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "colNames = data.columns\n",
    "for colName in colNames:\n",
    "    data = data.withColumn(colName, col(colName).cast('double'))\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "falling-wheat",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import stddev, mean\n",
    "mean_amount, sttdev_amount = data.select(mean(\"amount\"), stddev(\"amount\")).first()\n",
    "data=data.withColumn(\"amount_scaled\", (col(\"amount\") - mean_amount) / sttdev_amount)\n",
    "mean_time, sttdev_time = data.select(mean(\"time\"), stddev(\"time\")).first()\n",
    "data=data.withColumn(\"time_scaled\", (col(\"time\") - mean_time) / sttdev_time)\n",
    "data=data.drop(\"time\",\"amount\")\n",
    "data=data.withColumnRenamed('time_scaled', 'time')\n",
    "data=data.withColumnRenamed('amount_scaled', 'amount')\n",
    "print('success')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "moral-winner",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "df_assembler = VectorAssembler(inputCols=['time','amount','v1','v2','v3','v4','v5','v6','v7','v8','v9'\n",
    "                                          ,'v10','v11','v12','v13','v14'\n",
    "                                         ,'v15','v16','v17','v18','v19','v20'\n",
    "                                         ,'v21','v22','v23','v24','v25','v26','v27','v28'],outputCol='features')\n",
    "data= df_assembler.transform(data)\n",
    "(train, test) = data.randomSplit([0.8, 0.2],seed = 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "lined-championship",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "df_assembler = VectorAssembler(inputCols=['time','amount','v1','v2','v3','v4','v5','v6','v7','v8','v9'\n",
    "                                          ,'v10','v11','v12','v13','v14'\n",
    "                                         ,'v15','v16','v17','v18','v19','v20'\n",
    "                                         ,'v21','v22','v23','v24','v25','v26','v27','v28'],outputCol='features')\n",
    "df= df_assembler.transform(df)\n",
    "(train1, test1) = df.randomSplit([0.8, 0.2],seed = 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "central-phone",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pyspark.sql.functions as f\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"class\", rawPredictionCol=\"prediction\", metricName='areaUnderROC')\n",
    "def precise_recall_f1(prediction):\n",
    "    tp = prediction.filter((f.col('class')  == 1) & (f.col('prediction')  == 1)).count()\n",
    "    tn = prediction.filter((f.col('class')  == 0) & (f.col('prediction')  == 0)).count()\n",
    "    fp = prediction.filter((f.col('class')  == 0) & (f.col('prediction')  == 1)).count()\n",
    "    fn = prediction.filter((f.col('class')  == 1) & (f.col('prediction')  == 0)).count()\n",
    "    try:\n",
    "        acc=float(tp+tn)/(tp+tn+fp+fn)\n",
    "    except:\n",
    "        acc=0\n",
    "    try:\n",
    "        p = float(tp)/(tp + fp)\n",
    "    except:\n",
    "        p = 0\n",
    "    try:\n",
    "        r = float(tp)/(tp + fn)\n",
    "    except:\n",
    "        r = 0\n",
    "    try:\n",
    "        f1 = 2*p*r/(p+r)\n",
    "    except:\n",
    "        f1 = 0\n",
    "    result=[acc,p,r,f1]\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "planned-transsexual",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22905"
     ]
    }
   ],
   "source": [
    "train.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "honest-queens",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "An error occurred while calling o834.count.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 111 in stage 19.0 failed 4 times, most recent failure: Lost task 111.3 in stage 19.0 (TID 1141, 172.16.11.183, executor 0): org.apache.spark.SparkException: Failed to execute user defined function($anonfun$3: (struct<time:double,amount:double,v1:double,v2:double,v3:double,v4:double,v5:double,v6:double,v7:double,v8:double,v9:double,v10:double,v11:double,v12:double,v13:double,v14:double,v15:double,v16:double,v17:double,v18:double,v19:double,v20:double,v21:double,v22:double,... 6 more fields>) => vector)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.agg_ScalaUDF$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.agg_doAggregateWithKeysOutput$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.sort_addToSorter$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.agg_doAggregateWithoutKey$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$1.hasNext(WholeStageCodegenExec.scala:614)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.spark.SparkException: Values to assemble cannot be null.\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$assemble$1.apply(VectorAssembler.scala:163)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$assemble$1.apply(VectorAssembler.scala:146)\n",
      "\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:146)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$3.apply(VectorAssembler.scala:99)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$3.apply(VectorAssembler.scala:98)\n",
      "\t... 16 more\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1599)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1587)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1586)\n",
      "\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1586)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n",
      "\tat scala.Option.foreach(Option.scala:257)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1820)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1769)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1758)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2027)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2048)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2067)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2092)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:939)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n",
      "\tat org.apache.spark.rdd.RDD.collect(RDD.scala:938)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:297)\n",
      "\tat org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2770)\n",
      "\tat org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2769)\n",
      "\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3253)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\n",
      "\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3252)\n",
      "\tat org.apache.spark.sql.Dataset.count(Dataset.scala:2769)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.spark.SparkException: Failed to execute user defined function($anonfun$3: (struct<time:double,amount:double,v1:double,v2:double,v3:double,v4:double,v5:double,v6:double,v7:double,v8:double,v9:double,v10:double,v11:double,v12:double,v13:double,v14:double,v15:double,v16:double,v17:double,v18:double,v19:double,v20:double,v21:double,v22:double,... 6 more fields>) => vector)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.agg_ScalaUDF$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.agg_doAggregateWithKeysOutput$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.sort_addToSorter$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.agg_doAggregateWithoutKey$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$1.hasNext(WholeStageCodegenExec.scala:614)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "Caused by: org.apache.spark.SparkException: Values to assemble cannot be null.\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$assemble$1.apply(VectorAssembler.scala:163)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$assemble$1.apply(VectorAssembler.scala:146)\n",
      "\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:146)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$3.apply(VectorAssembler.scala:99)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$3.apply(VectorAssembler.scala:98)\n",
      "\t... 16 more\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py\", line 455, in count\n",
      "    return int(self._jdf.count())\n",
      "  File \"/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 63, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 328, in get_return_value\n",
      "    format(target_id, \".\", name), value)\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o834.count.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 111 in stage 19.0 failed 4 times, most recent failure: Lost task 111.3 in stage 19.0 (TID 1141, 172.16.11.183, executor 0): org.apache.spark.SparkException: Failed to execute user defined function($anonfun$3: (struct<time:double,amount:double,v1:double,v2:double,v3:double,v4:double,v5:double,v6:double,v7:double,v8:double,v9:double,v10:double,v11:double,v12:double,v13:double,v14:double,v15:double,v16:double,v17:double,v18:double,v19:double,v20:double,v21:double,v22:double,... 6 more fields>) => vector)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.agg_ScalaUDF$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.agg_doAggregateWithKeysOutput$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.sort_addToSorter$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.agg_doAggregateWithoutKey$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$1.hasNext(WholeStageCodegenExec.scala:614)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.spark.SparkException: Values to assemble cannot be null.\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$assemble$1.apply(VectorAssembler.scala:163)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$assemble$1.apply(VectorAssembler.scala:146)\n",
      "\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:146)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$3.apply(VectorAssembler.scala:99)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$3.apply(VectorAssembler.scala:98)\n",
      "\t... 16 more\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1599)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1587)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1586)\n",
      "\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1586)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n",
      "\tat scala.Option.foreach(Option.scala:257)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1820)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1769)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1758)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2027)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2048)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2067)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2092)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:939)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n",
      "\tat org.apache.spark.rdd.RDD.collect(RDD.scala:938)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:297)\n",
      "\tat org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2770)\n",
      "\tat org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2769)\n",
      "\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3253)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\n",
      "\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3252)\n",
      "\tat org.apache.spark.sql.Dataset.count(Dataset.scala:2769)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.spark.SparkException: Failed to execute user defined function($anonfun$3: (struct<time:double,amount:double,v1:double,v2:double,v3:double,v4:double,v5:double,v6:double,v7:double,v8:double,v9:double,v10:double,v11:double,v12:double,v13:double,v14:double,v15:double,v16:double,v17:double,v18:double,v19:double,v20:double,v21:double,v22:double,... 6 more fields>) => vector)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.agg_ScalaUDF$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.agg_doAggregateWithKeysOutput$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.sort_addToSorter$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.agg_doAggregateWithoutKey$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$1.hasNext(WholeStageCodegenExec.scala:614)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "Caused by: org.apache.spark.SparkException: Values to assemble cannot be null.\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$assemble$1.apply(VectorAssembler.scala:163)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$assemble$1.apply(VectorAssembler.scala:146)\n",
      "\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$.assemble(VectorAssembler.scala:146)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$3.apply(VectorAssembler.scala:99)\n",
      "\tat org.apache.spark.ml.feature.VectorAssembler$$anonfun$3.apply(VectorAssembler.scala:98)\n",
      "\t... 16 more\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "apparent-laundry",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lrresult ['lr', 0.9992784357895849, 0.8676470588235294, 0.6483516483516484, 0.7421383647798743, 0.8240965010663582]"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(featuresCol='features',labelCol = 'class').fit(train)\n",
    "lrpredictions= lr.transform(test)\n",
    "lrresult=['lr']\n",
    "lrresult1=precise_recall_f1(lrpredictions)\n",
    "lrresult=lrresult+lrresult1\n",
    "lrroc=evaluator.evaluate(lrpredictions)\n",
    "lrresult.append(lrroc)\n",
    "print(\"lrresult\",lrresult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "integral-rebound",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "residential-house",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "express-borough",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "formal-journey",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "religious-mechanics",
   "metadata": {},
   "source": [
    "## 数据处理：1.处理重复数据\n",
    "- 整行去重：data.distinct()这个方程可以对于完全相同的行进行去重。\n",
    "- data.count()方程会打印出这个列表的行数，以便于您了解去重之后的数据集行数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "million-jerusalem",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.distinct()\n",
    "data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reasonable-proxy",
   "metadata": {},
   "source": [
    "## 数据处理：2.处理空缺数据\n",
    "- 运用agg方程算出每列空缺的数据比\n",
    "- 删除掉结果项空缺的数据（data.na.drop(subset=['Class'])），这个数据分析主要是对于“Class”项，当Class缺失，该行对于后续分析则没有帮助。\n",
    "- 删除掉空缺项过多的行。您可以通过利用thresh参数，为每一行缺失数据的数量指定五个阈值，从而限定要删除的行。以下我们选择的阈值为5，则删除掉空缺项等于或大约5的行。您可以改变thresh对应的值来改变阈值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "removable-faculty",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "data.agg(*[(1-(f.count(c) /f.count('*'))).alias(c+'_missing') for c in data.columns]).show()\n",
    "data=data.na.drop(subset=['class'])\n",
    "data=data.dropna(thresh=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "future-brother",
   "metadata": {},
   "source": [
    "## 数据处理：3.将某列数据的类别进行转化\n",
    "- 当您直接用spark的read_csv导入数据时，生成的dataframe往往会默认每列的类别为string，然而string类别会让您接下来的数据分析以及训练的步骤报错。\n",
    "- 以下这个单元格将四列的数据种类进行了改变。您可以通过在withColumn这个方程来进行改变，在括号中填入您想要改变的列名，以及想改变成的种类形式。形式：（“列名”,dataframe[\"列名\"].cast('type')）\n",
    "- 以下为一些您可能需要使用的类别：</br>binary;boolean;int;string;float;timestamp;date;float</br>您可以根据数据种类将以上类别替换到'type'里。\n",
    "- 直接运行dataframe的名称，则会有每列的类型，根据现实地数据类型进行改变"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "historic-cricket",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dramatic-promotion",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "colNames = data.columns\n",
    "for colName in colNames:\n",
    "    data = data.withColumn(colName, col(colName).cast('double'))\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caroline-liability",
   "metadata": {},
   "source": [
    "## 数据处理： 5.标准化\n",
    "- 将数据按期属性（按列进行）减去其均值，并处以其方差。得到的结果是，对于每个属性/每列来说所有数据都聚集在0附近，方差为1。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "animal-round",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import stddev, mean\n",
    "mean_amount, sttdev_amount = data.select(mean(\"amount\"), stddev(\"amount\")).first()\n",
    "data=data.withColumn(\"amount_scaled\", (col(\"amount\") - mean_amount) / sttdev_amount)\n",
    "mean_time, sttdev_time = data.select(mean(\"time\"), stddev(\"time\")).first()\n",
    "data=data.withColumn(\"time_scaled\", (col(\"time\") - mean_time) / sttdev_time)\n",
    "data=data.drop(\"time\",\"amount\")\n",
    "data=data.withColumnRenamed('time_scaled', 'time')\n",
    "data=data.withColumnRenamed('amount_scaled', 'amount')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "congressional-interstate",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.write.format(\"hive\").mode(\"overwrite\").saveAsTable(\"user_erin.creditcard\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forced-complexity",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
